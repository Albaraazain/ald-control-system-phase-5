#!/usr/bin/env python3
"""
Comprehensive Test Suite - Master Test Orchestrator
Coordinates all test agents and validates the entire ALD control system
"""

import asyncio
import logging
import json
import time
import sys
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from uuid import uuid4
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, asdict
import traceback

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from test_environment_setup import test_env
from database.database import DatabaseConnection
from log_setup import setup_logger

@dataclass
class TestResult:
    """Test result data structure"""
    test_id: str
    test_name: str
    category: str
    status: str  # pending, running, passed, failed, skipped
    start_time: datetime
    end_time: Optional[datetime] = None
    duration_seconds: Optional[float] = None
    error_message: Optional[str] = None
    details: Optional[Dict[str, Any]] = None
    artifacts: List[str] = None
    
    def __post_init__(self):
        if self.artifacts is None:
            self.artifacts = []

@dataclass
class TestSuiteMetrics:
    """Test suite execution metrics"""
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0
    skipped_tests: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    total_duration_seconds: Optional[float] = None
    
    @property
    def success_rate(self) -> float:
        if self.total_tests == 0:
            return 0.0
        return self.passed_tests / self.total_tests
    
    @property
    def status(self) -> str:
        if self.failed_tests > 0:
            return "FAILED"
        elif self.passed_tests == self.total_tests:
            return "PASSED"
        else:
            return "PARTIAL"

class ComprehensiveTestSuite:
    """Master test orchestrator for complete system validation"""
    
    def __init__(self):
        self.logger = setup_logger(__name__)
        self.suite_id = str(uuid4())
        self.results: List[TestResult] = []
        self.metrics = TestSuiteMetrics()
        self.db = None
        self.test_workspace = None
        
    async def initialize(self) -> bool:
        """Initialize test suite and environment"""
        self.logger.info("ðŸš€ Initializing Comprehensive Test Suite...")
        
        try:
            # Initialize test environment
            env_info = await test_env.initialize_environment()
            if env_info["status"] != "ready":
                raise RuntimeError("Test environment initialization failed")
            
            self.test_workspace = Path(env_info["workspace"])
            self.db = test_env.db
            
            self.logger.info(f"âœ… Test suite initialized - Session: {test_env.test_session_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Test suite initialization failed: {e}")
            return False
    
    async def run_complete_test_suite(self) -> TestSuiteMetrics:
        """Execute comprehensive test suite"""
        self.logger.info("ðŸ§ª Starting Comprehensive Test Suite Execution...")
        
        self.metrics.start_time = datetime.now()
        
        try:
            # Define test execution plan
            test_plan = [
                ("Database Schema Validation", self._run_schema_validation_tests),
                ("Database Integration Tests", self._run_database_integration_tests),
                ("Recipe Creation and Validation", self._run_recipe_validation_tests),
                ("Command Processing Tests", self._run_command_processing_tests),
                ("Step Execution Tests", self._run_step_execution_tests),
                ("End-to-End Workflow Tests", self._run_e2e_workflow_tests),
                ("Performance and Load Tests", self._run_performance_tests),
                ("Error Handling Tests", self._run_error_handling_tests),
                ("Concurrency Tests", self._run_concurrency_tests),
                ("Production Readiness Validation", self._run_production_readiness_tests)
            ]
            
            # Execute test categories
            for category_name, test_func in test_plan:
                self.logger.info(f"ðŸ”„ Running {category_name}...")
                
                try:
                    category_results = await test_func()
                    self.results.extend(category_results)
                    
                    passed = sum(1 for r in category_results if r.status == "passed")
                    failed = sum(1 for r in category_results if r.status == "failed")
                    skipped = sum(1 for r in category_results if r.status == "skipped")
                    
                    self.logger.info(f"âœ… {category_name} completed: {passed} passed, {failed} failed, {skipped} skipped")
                    
                except Exception as e:
                    self.logger.error(f"âŒ {category_name} failed: {e}")
                    # Add failure result
                    failure_result = TestResult(
                        test_id=str(uuid4()),
                        test_name=f"{category_name} - Category Execution",
                        category=category_name,
                        status="failed",
                        start_time=datetime.now(),
                        error_message=str(e)
                    )
                    failure_result.end_time = datetime.now()
                    failure_result.duration_seconds = 0
                    self.results.append(failure_result)
            
            # Calculate final metrics
            self._calculate_final_metrics()
            
            # Generate comprehensive report
            await self._generate_comprehensive_report()
            
            # Create test dashboard
            await self._create_test_dashboard()
            
            self.logger.info(f"ðŸŽ¯ Test Suite Complete: {self.metrics.status}")
            
        except Exception as e:
            self.logger.error(f"âŒ Test suite execution failed: {e}")
            self.metrics.end_time = datetime.now()
        
        return self.metrics
    
    async def _run_schema_validation_tests(self) -> List[TestResult]:
        """Run database schema validation tests"""
        results = []
        
        # Test 1: Schema Structure Validation
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Database Schema Structure Validation",
            category="Schema Validation",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            # Check critical tables exist with proper structure
            critical_tables = [
                "recipes", "recipe_steps", "recipe_parameters",
                "valve_step_config", "purge_step_config", "loop_step_config",
                "process_executions", "process_execution_state",
                "machines", "machine_components", "component_parameters"
            ]
            
            schema_issues = []
            
            for table_name in critical_tables:
                try:
                    result = await self.db.execute_query(f"""
                        SELECT column_name, data_type, is_nullable, column_default
                        FROM information_schema.columns 
                        WHERE table_name = '{table_name}' 
                        AND table_schema = 'public'
                        ORDER BY ordinal_position
                    """)
                    
                    if not result:
                        schema_issues.append(f"Table {table_name} not found")
                    else:
                        # Validate specific column requirements
                        columns = {col["column_name"]: col for col in result}
                        
                        # Check for required columns based on table
                        required_cols = self._get_required_columns(table_name)
                        for req_col in required_cols:
                            if req_col not in columns:
                                schema_issues.append(f"Table {table_name} missing required column: {req_col}")
                
                except Exception as e:
                    schema_issues.append(f"Error checking table {table_name}: {str(e)}")
            
            test_result.details = {
                "tables_checked": len(critical_tables),
                "schema_issues": schema_issues,
                "issues_count": len(schema_issues)
            }
            
            if schema_issues:
                test_result.status = "failed"
                test_result.error_message = f"Schema validation failed with {len(schema_issues)} issues"
            else:
                test_result.status = "passed"
            
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        results.append(test_result)
        
        # Test 2: Foreign Key Integrity
        fk_test = await self._test_foreign_key_integrity()
        results.append(fk_test)
        
        # Test 3: Data Type Consistency
        dt_test = await self._test_data_type_consistency()
        results.append(dt_test)
        
        return results
    
    async def _run_database_integration_tests(self) -> List[TestResult]:
        """Run database integration tests"""
        results = []
        
        # Test 1: Recipe-Step Configuration Integration
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Recipe-Step Configuration Integration",
            category="Database Integration",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            # Check that all recipe steps have proper configurations
            result = await self.db.execute_query("""
                SELECT 
                    rs.id as step_id,
                    rs.name,
                    rs.type,
                    CASE 
                        WHEN rs.type = 'valve' AND vsc.step_id IS NOT NULL THEN true
                        WHEN rs.type = 'purge' AND psc.step_id IS NOT NULL THEN true
                        WHEN rs.type = 'loop' AND lsc.step_id IS NOT NULL THEN true
                        WHEN rs.type = 'parameter' THEN true
                        ELSE false
                    END as has_config
                FROM recipe_steps rs
                LEFT JOIN valve_step_config vsc ON rs.id = vsc.step_id
                LEFT JOIN purge_step_config psc ON rs.id = psc.step_id  
                LEFT JOIN loop_step_config lsc ON rs.id = lsc.step_id
                WHERE rs.type IN ('valve', 'purge', 'loop')
            """)
            
            missing_configs = [row for row in result if not row["has_config"]]
            
            test_result.details = {
                "total_steps_checked": len(result),
                "steps_with_configs": len(result) - len(missing_configs),
                "missing_configs": len(missing_configs),
                "missing_config_details": missing_configs
            }
            
            if missing_configs:
                test_result.status = "failed"
                test_result.error_message = f"{len(missing_configs)} recipe steps missing required configurations"
            else:
                test_result.status = "passed"
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        results.append(test_result)
        
        # Test 2: Process Execution State Integrity
        pe_test = await self._test_process_execution_state_integrity()
        results.append(pe_test)
        
        # Test 3: Component Parameter Linkage
        cp_test = await self._test_component_parameter_linkage()
        results.append(cp_test)
        
        return results
    
    async def _run_recipe_validation_tests(self) -> List[TestResult]:
        """Run recipe creation and validation tests"""
        results = []
        
        # Get test recipes
        test_recipes = await test_env.get_test_recipes()
        
        for recipe in test_recipes[:3]:  # Test first 3 recipes
            test_result = TestResult(
                test_id=str(uuid4()),
                test_name=f"Recipe Validation - {recipe['name']}",
                category="Recipe Validation",
                status="running",
                start_time=datetime.now()
            )
            
            try:
                # Validate recipe structure
                recipe_validation = await self._validate_recipe_structure(recipe['id'])
                
                test_result.details = recipe_validation
                
                if recipe_validation["is_valid"]:
                    test_result.status = "passed"
                else:
                    test_result.status = "failed"
                    test_result.error_message = f"Recipe validation failed: {recipe_validation.get('issues', [])}"
            
            except Exception as e:
                test_result.status = "failed"
                test_result.error_message = str(e)
            
            test_result.end_time = datetime.now()
            test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
            results.append(test_result)
        
        return results
    
    async def _run_command_processing_tests(self) -> List[TestResult]:
        """Run command processing tests"""
        results = []
        
        # Test 1: Command Creation and Status Tracking
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Command Creation and Status Tracking",
            category="Command Processing",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            # Get test machines and recipes
            machines = await test_env.get_test_machines()
            recipes = await test_env.get_test_recipes()
            
            if not machines or not recipes:
                test_result.status = "skipped"
                test_result.error_message = "No test machines or recipes available"
            else:
                # Create test command
                machine_id = machines[0]["id"]
                recipe_id = recipes[0]["id"]
                
                # Insert test command
                command_id = str(uuid4())
                await self.db.execute_query("""
                    INSERT INTO recipe_commands (id, type, parameters, status, machine_id, recipe_step_id)
                    VALUES ($1, 'start_recipe', $2, 'pending', $3, null)
                """, [
                    command_id,
                    json.dumps({"recipe_id": recipe_id}),
                    machine_id
                ])
                
                # Verify command was created
                result = await self.db.execute_query(
                    "SELECT * FROM recipe_commands WHERE id = $1",
                    [command_id]
                )
                
                if result:
                    test_result.status = "passed"
                    test_result.details = {
                        "command_created": True,
                        "command_id": command_id,
                        "status": result[0]["status"]
                    }
                    
                    # Cleanup test command
                    await self.db.execute_query(
                        "DELETE FROM recipe_commands WHERE id = $1",
                        [command_id]
                    )
                else:
                    test_result.status = "failed"
                    test_result.error_message = "Command was not created in database"
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
            test_result.details = {"error_trace": traceback.format_exc()}
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        results.append(test_result)
        
        return results
    
    async def _run_step_execution_tests(self) -> List[TestResult]:
        """Run step execution tests"""
        results = []
        
        # Test step configuration loading for each type
        step_types = ["valve", "purge", "loop"]
        
        for step_type in step_types:
            test_result = TestResult(
                test_id=str(uuid4()),
                test_name=f"Step Configuration Loading - {step_type.title()}",
                category="Step Execution",
                status="running",
                start_time=datetime.now()
            )
            
            try:
                config_validation = await self._test_step_config_loading(step_type)
                
                test_result.details = config_validation
                
                if config_validation["success"]:
                    test_result.status = "passed"
                else:
                    test_result.status = "failed"
                    test_result.error_message = config_validation.get("error", "Unknown error")
            
            except Exception as e:
                test_result.status = "failed"
                test_result.error_message = str(e)
            
            test_result.end_time = datetime.now()
            test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
            results.append(test_result)
        
        return results
    
    async def _run_e2e_workflow_tests(self) -> List[TestResult]:
        """Run end-to-end workflow tests"""
        results = []
        
        # Test 1: Complete Recipe Execution Simulation
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Complete Recipe Execution Simulation",
            category="End-to-End Workflow",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            workflow_result = await self._simulate_complete_recipe_execution()
            
            test_result.details = workflow_result
            
            if workflow_result["success"]:
                test_result.status = "passed"
            else:
                test_result.status = "failed"
                test_result.error_message = workflow_result.get("error", "Workflow simulation failed")
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        results.append(test_result)
        
        return results
    
    async def _run_performance_tests(self) -> List[TestResult]:
        """Run performance and load tests"""
        results = []
        
        # Test 1: Database Query Performance
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Database Query Performance",
            category="Performance",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            performance_metrics = await self._measure_database_performance()
            
            test_result.details = performance_metrics
            
            # Check if performance meets thresholds
            slow_queries = [q for q in performance_metrics["query_times"] if q["duration_ms"] > 1000]
            
            if slow_queries:
                test_result.status = "failed"
                test_result.error_message = f"{len(slow_queries)} queries exceeded 1000ms threshold"
            else:
                test_result.status = "passed"
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        results.append(test_result)
        
        return results
    
    async def _run_error_handling_tests(self) -> List[TestResult]:
        """Run error handling and recovery tests"""
        results = []
        
        # Test 1: Invalid Data Handling
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Invalid Data Handling",
            category="Error Handling",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            error_handling_result = await self._test_invalid_data_handling()
            
            test_result.details = error_handling_result
            
            if error_handling_result["handles_errors_gracefully"]:
                test_result.status = "passed"
            else:
                test_result.status = "failed"
                test_result.error_message = "System does not handle invalid data gracefully"
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        results.append(test_result)
        
        return results
    
    async def _run_concurrency_tests(self) -> List[TestResult]:
        """Run concurrency tests"""
        results = []
        
        # Test 1: Concurrent Database Access
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Concurrent Database Access",
            category="Concurrency",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            concurrency_result = await self._test_concurrent_database_access()
            
            test_result.details = concurrency_result
            
            if concurrency_result["no_deadlocks"] and concurrency_result["data_consistency"]:
                test_result.status = "passed"
            else:
                test_result.status = "failed"
                test_result.error_message = "Concurrency issues detected"
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        results.append(test_result)
        
        return results
    
    async def _run_production_readiness_tests(self) -> List[TestResult]:
        """Run production readiness validation tests"""
        results = []
        
        # Test 1: System Health Check
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="System Health Check",
            category="Production Readiness",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            health_check = await self._perform_system_health_check()
            
            test_result.details = health_check
            
            if health_check["overall_health"] == "healthy":
                test_result.status = "passed"
            else:
                test_result.status = "failed"
                test_result.error_message = f"System health: {health_check['overall_health']}"
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        results.append(test_result)
        
        return results
    
    def _calculate_final_metrics(self):
        """Calculate final test suite metrics"""
        self.metrics.end_time = datetime.now()
        self.metrics.total_duration_seconds = (self.metrics.end_time - self.metrics.start_time).total_seconds()
        
        self.metrics.total_tests = len(self.results)
        self.metrics.passed_tests = sum(1 for r in self.results if r.status == "passed")
        self.metrics.failed_tests = sum(1 for r in self.results if r.status == "failed")
        self.metrics.skipped_tests = sum(1 for r in self.results if r.status == "skipped")
    
    async def _generate_comprehensive_report(self):
        """Generate comprehensive test report"""
        report = {
            "suite_id": self.suite_id,
            "execution_info": {
                "start_time": self.metrics.start_time.isoformat(),
                "end_time": self.metrics.end_time.isoformat() if self.metrics.end_time else None,
                "duration_seconds": self.metrics.total_duration_seconds,
                "test_environment": test_env.get_session_info()
            },
            "summary": asdict(self.metrics),
            "results_by_category": {},
            "detailed_results": [asdict(r) for r in self.results],
            "recommendations": [],
            "production_readiness": {
                "status": "READY" if self.metrics.success_rate >= 0.95 else "NOT_READY",
                "critical_issues": [r for r in self.results if r.status == "failed" and r.category in ["Schema Validation", "Database Integration"]],
                "success_rate": self.metrics.success_rate
            }
        }
        
        # Group results by category
        for result in self.results:
            category = result.category
            if category not in report["results_by_category"]:
                report["results_by_category"][category] = {
                    "total": 0, "passed": 0, "failed": 0, "skipped": 0
                }
            
            report["results_by_category"][category]["total"] += 1
            report["results_by_category"][category][result.status] += 1
        
        # Generate recommendations
        if self.metrics.failed_tests > 0:
            report["recommendations"].append("Address all failed tests before production deployment")
        
        if self.metrics.success_rate < 0.95:
            report["recommendations"].append("Improve test success rate to at least 95% for production readiness")
        
        # Save report
        report_file = self.test_workspace / "reports" / f"comprehensive_test_report_{self.suite_id}.json"
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2, default=str)
        
        self.logger.info(f"ðŸ“Š Comprehensive report saved: {report_file}")
    
    async def _create_test_dashboard(self):
        """Create real-time test execution dashboard"""
        dashboard = {
            "suite_id": self.suite_id,
            "last_updated": datetime.now().isoformat(),
            "metrics": asdict(self.metrics),
            "category_status": {},
            "recent_results": [asdict(r) for r in self.results[-10:]],
            "alerts": []
        }
        
        # Category status
        categories = set(r.category for r in self.results)
        for category in categories:
            category_results = [r for r in self.results if r.category == category]
            dashboard["category_status"][category] = {
                "total": len(category_results),
                "passed": sum(1 for r in category_results if r.status == "passed"),
                "failed": sum(1 for r in category_results if r.status == "failed"),
                "status": "FAILED" if any(r.status == "failed" for r in category_results) else "PASSED"
            }
        
        # Generate alerts
        if self.metrics.failed_tests > 0:
            dashboard["alerts"].append({
                "level": "ERROR",
                "message": f"{self.metrics.failed_tests} tests failed",
                "timestamp": datetime.now().isoformat()
            })
        
        # Save dashboard
        dashboard_file = self.test_workspace / "reports" / "test_execution_dashboard.json"
        with open(dashboard_file, "w") as f:
            json.dump(dashboard, f, indent=2, default=str)
        
        self.logger.info(f"ðŸ“ˆ Test dashboard created: {dashboard_file}")
    
    # Helper methods for specific test implementations
    def _get_required_columns(self, table_name: str) -> List[str]:
        """Get required columns for table validation"""
        required_columns = {
            "recipes": ["id", "name", "machine_type", "created_by"],
            "recipe_steps": ["id", "recipe_id", "sequence_number", "name", "type"],
            "valve_step_config": ["id", "step_id", "valve_number", "duration_ms"],
            "purge_step_config": ["id", "step_id", "duration_ms"],
            "loop_step_config": ["id", "step_id", "iteration_count"],
            "process_executions": ["id", "recipe_id", "machine_id", "status"],
            "process_execution_state": ["id", "execution_id"]
        }
        return required_columns.get(table_name, ["id"])
    
    async def _test_foreign_key_integrity(self) -> TestResult:
        """Test foreign key integrity across the database"""
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Foreign Key Integrity Check",
            category="Schema Validation",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            integrity_issues = []
            
            # Check critical foreign key relationships
            fk_checks = [
                ("recipe_steps", "recipe_id", "recipes", "id"),
                ("valve_step_config", "step_id", "recipe_steps", "id"),
                ("purge_step_config", "step_id", "recipe_steps", "id"),
                ("loop_step_config", "step_id", "recipe_steps", "id"),
                ("process_execution_state", "execution_id", "process_executions", "id")
            ]
            
            for child_table, child_col, parent_table, parent_col in fk_checks:
                result = await self.db.execute_query(f"""
                    SELECT COUNT(*) as orphaned_count
                    FROM {child_table} c
                    LEFT JOIN {parent_table} p ON c.{child_col} = p.{parent_col}
                    WHERE p.{parent_col} IS NULL AND c.{child_col} IS NOT NULL
                """)
                
                orphaned = result[0]["orphaned_count"]
                if orphaned > 0:
                    integrity_issues.append({
                        "relationship": f"{child_table}.{child_col} -> {parent_table}.{parent_col}",
                        "orphaned_records": orphaned
                    })
            
            test_result.details = {
                "checks_performed": len(fk_checks),
                "integrity_issues": integrity_issues
            }
            
            if integrity_issues:
                test_result.status = "failed"
                test_result.error_message = f"Found {len(integrity_issues)} foreign key integrity issues"
            else:
                test_result.status = "passed"
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        
        return test_result
    
    async def _test_data_type_consistency(self) -> TestResult:
        """Test data type consistency across related tables"""
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Data Type Consistency Check",
            category="Schema Validation",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            # This is a simplified check - in practice you'd validate specific type relationships
            test_result.status = "passed"
            test_result.details = {"consistency_check": "basic_validation_passed"}
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        
        return test_result
    
    async def _test_process_execution_state_integrity(self) -> TestResult:
        """Test process execution state integrity"""
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Process Execution State Integrity",
            category="Database Integration",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            # Check that all process executions have corresponding state records
            result = await self.db.execute_query("""
                SELECT 
                    COUNT(pe.id) as total_executions,
                    COUNT(pes.execution_id) as executions_with_state
                FROM process_executions pe
                LEFT JOIN process_execution_state pes ON pe.id = pes.execution_id
            """)
            
            total = result[0]["total_executions"]
            with_state = result[0]["executions_with_state"]
            
            test_result.details = {
                "total_executions": total,
                "executions_with_state": with_state,
                "missing_state_records": total - with_state
            }
            
            if total == with_state:
                test_result.status = "passed"
            else:
                test_result.status = "failed"
                test_result.error_message = f"{total - with_state} process executions missing state records"
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        
        return test_result
    
    async def _test_component_parameter_linkage(self) -> TestResult:
        """Test component parameter linkage"""
        test_result = TestResult(
            test_id=str(uuid4()),
            test_name="Component Parameter Linkage",
            category="Database Integration",
            status="running",
            start_time=datetime.now()
        )
        
        try:
            # Check component parameter definitions are properly linked
            result = await self.db.execute_query("""
                SELECT 
                    COUNT(cp.id) as total_parameters,
                    COUNT(cpd.id) as parameters_with_definitions
                FROM component_parameters cp
                LEFT JOIN component_parameter_definitions cpd ON cp.definition_id = cpd.id
            """)
            
            total = result[0]["total_parameters"] 
            with_defs = result[0]["parameters_with_definitions"]
            
            test_result.details = {
                "total_parameters": total,
                "parameters_with_definitions": with_defs,
                "missing_definitions": total - with_defs
            }
            
            if total == with_defs:
                test_result.status = "passed"
            else:
                test_result.status = "failed"
                test_result.error_message = f"{total - with_defs} parameters missing definition links"
        
        except Exception as e:
            test_result.status = "failed"
            test_result.error_message = str(e)
        
        test_result.end_time = datetime.now()
        test_result.duration_seconds = (test_result.end_time - test_result.start_time).total_seconds()
        
        return test_result
    
    async def _validate_recipe_structure(self, recipe_id: str) -> Dict[str, Any]:
        """Validate a recipe's structure and configuration"""
        try:
            # Get recipe steps
            steps_result = await self.db.execute_query("""
                SELECT id, name, type, sequence_number
                FROM recipe_steps 
                WHERE recipe_id = $1 
                ORDER BY sequence_number
            """, [recipe_id])
            
            validation_issues = []
            step_configs = {}
            
            for step in steps_result:
                step_id = step["id"]
                step_type = step["type"]
                
                # Check if step has appropriate configuration
                if step_type == "valve":
                    config_result = await self.db.execute_query(
                        "SELECT * FROM valve_step_config WHERE step_id = $1", [step_id]
                    )
                    if config_result:
                        step_configs[step_id] = {"type": "valve", "config": config_result[0]}
                    else:
                        validation_issues.append(f"Valve step {step['name']} missing configuration")
                
                elif step_type == "purge":
                    config_result = await self.db.execute_query(
                        "SELECT * FROM purge_step_config WHERE step_id = $1", [step_id]
                    )
                    if config_result:
                        step_configs[step_id] = {"type": "purge", "config": config_result[0]}
                    else:
                        validation_issues.append(f"Purge step {step['name']} missing configuration")
                
                elif step_type == "loop":
                    config_result = await self.db.execute_query(
                        "SELECT * FROM loop_step_config WHERE step_id = $1", [step_id]
                    )
                    if config_result:
                        step_configs[step_id] = {"type": "loop", "config": config_result[0]}
                    else:
                        validation_issues.append(f"Loop step {step['name']} missing configuration")
            
            return {
                "is_valid": len(validation_issues) == 0,
                "total_steps": len(steps_result),
                "configured_steps": len(step_configs),
                "issues": validation_issues,
                "step_configs": step_configs
            }
        
        except Exception as e:
            return {
                "is_valid": False,
                "error": str(e)
            }
    
    async def _test_step_config_loading(self, step_type: str) -> Dict[str, Any]:
        """Test step configuration loading for a specific step type"""
        try:
            table_map = {
                "valve": "valve_step_config",
                "purge": "purge_step_config", 
                "loop": "loop_step_config"
            }
            
            config_table = table_map[step_type]
            
            # Get sample configurations
            result = await self.db.execute_query(f"""
                SELECT sc.*, rs.name as step_name, rs.type
                FROM {config_table} sc
                JOIN recipe_steps rs ON sc.step_id = rs.id
                LIMIT 3
            """)
            
            if not result:
                return {"success": False, "error": f"No {step_type} step configurations found"}
            
            # Validate configuration structure
            for config in result:
                required_fields = {
                    "valve": ["valve_number", "duration_ms"],
                    "purge": ["duration_ms"],
                    "loop": ["iteration_count"]
                }
                
                missing_fields = []
                for field in required_fields[step_type]:
                    if field not in config or config[field] is None:
                        missing_fields.append(field)
                
                if missing_fields:
                    return {
                        "success": False,
                        "error": f"Configuration missing required fields: {missing_fields}"
                    }
            
            return {
                "success": True,
                "configurations_tested": len(result),
                "sample_configs": result
            }
        
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _simulate_complete_recipe_execution(self) -> Dict[str, Any]:
        """Simulate a complete recipe execution workflow"""
        try:
            # Get test data
            machines = await test_env.get_test_machines()
            recipes = await test_env.get_test_recipes()
            
            if not machines or not recipes:
                return {"success": False, "error": "No test machines or recipes available"}
            
            machine_id = machines[0]["id"]
            recipe_id = recipes[0]["id"]
            
            # Create test process execution
            process_id = str(uuid4())
            session_id = str(uuid4())
            
            # Insert process execution
            await self.db.execute_query("""
                INSERT INTO process_executions (
                    id, machine_id, recipe_id, status, start_time, 
                    operator_id, parameters, session_id
                ) VALUES ($1, $2, $3, 'preparing', now(), 
                         (SELECT id FROM auth.users LIMIT 1), '{}', $4)
            """, [process_id, machine_id, recipe_id, session_id])
            
            # Insert process execution state
            await self.db.execute_query("""
                INSERT INTO process_execution_state (execution_id, current_step_index, progress)
                VALUES ($1, 0, '{"total_steps": 0, "completed_steps": 0}')
            """, [process_id])
            
            # Simulate state updates
            await self.db.execute_query("""
                UPDATE process_executions 
                SET status = 'running', start_time = now()
                WHERE id = $1
            """, [process_id])
            
            await self.db.execute_query("""
                UPDATE process_execution_state 
                SET current_step_index = 1, 
                    current_overall_step = 1,
                    progress = '{"total_steps": 5, "completed_steps": 1}'
                WHERE execution_id = $1
            """, [process_id])
            
            # Complete simulation
            await self.db.execute_query("""
                UPDATE process_executions 
                SET status = 'completed', end_time = now()
                WHERE id = $1
            """, [process_id])
            
            # Verify final state
            result = await self.db.execute_query("""
                SELECT pe.status, pes.current_step_index, pes.progress
                FROM process_executions pe
                JOIN process_execution_state pes ON pe.id = pes.execution_id
                WHERE pe.id = $1
            """, [process_id])
            
            # Cleanup
            await self.db.execute_query("DELETE FROM process_execution_state WHERE execution_id = $1", [process_id])
            await self.db.execute_query("DELETE FROM process_executions WHERE id = $1", [process_id])
            
            if result:
                return {
                    "success": True,
                    "final_status": result[0]["status"],
                    "final_step_index": result[0]["current_step_index"],
                    "final_progress": result[0]["progress"]
                }
            else:
                return {"success": False, "error": "Process execution not found after simulation"}
        
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _measure_database_performance(self) -> Dict[str, Any]:
        """Measure database query performance"""
        query_tests = [
            ("Simple Recipe Query", "SELECT * FROM recipes LIMIT 10"),
            ("Recipe with Steps Join", """
                SELECT r.name, COUNT(rs.id) as step_count
                FROM recipes r
                LEFT JOIN recipe_steps rs ON r.id = rs.recipe_id
                GROUP BY r.id, r.name
                LIMIT 10
            """),
            ("Process Execution State Join", """
                SELECT pe.id, pe.status, pes.current_step_index
                FROM process_executions pe
                JOIN process_execution_state pes ON pe.id = pes.execution_id
                LIMIT 10
            """),
            ("Complex Recipe Configuration Query", """
                SELECT 
                    r.name,
                    rs.name as step_name,
                    rs.type,
                    COALESCE(vsc.duration_ms, psc.duration_ms) as duration_ms
                FROM recipes r
                JOIN recipe_steps rs ON r.id = rs.recipe_id
                LEFT JOIN valve_step_config vsc ON rs.id = vsc.step_id
                LEFT JOIN purge_step_config psc ON rs.id = psc.step_id
                WHERE r.name LIKE '%Test%'
                LIMIT 20
            """)
        ]
        
        query_times = []
        
        for query_name, query_sql in query_tests:
            start_time = time.time()
            try:
                await self.db.execute_query(query_sql)
                end_time = time.time()
                duration_ms = (end_time - start_time) * 1000
                
                query_times.append({
                    "query_name": query_name,
                    "duration_ms": duration_ms,
                    "status": "success"
                })
            except Exception as e:
                query_times.append({
                    "query_name": query_name,
                    "duration_ms": 0,
                    "status": "error",
                    "error": str(e)
                })
        
        return {
            "query_times": query_times,
            "average_duration_ms": sum(q["duration_ms"] for q in query_times) / len(query_times),
            "max_duration_ms": max(q["duration_ms"] for q in query_times),
            "performance_summary": "acceptable" if all(q["duration_ms"] < 1000 for q in query_times) else "needs_optimization"
        }
    
    async def _test_invalid_data_handling(self) -> Dict[str, Any]:
        """Test how the system handles invalid data"""
        try:
            # Test invalid recipe creation
            test_cases = []
            
            # Test 1: Invalid recipe data
            try:
                await self.db.execute_query("""
                    INSERT INTO recipes (name, machine_type, created_by)
                    VALUES (NULL, 'invalid_type', 'invalid_user_id')
                """)
                test_cases.append({"test": "invalid_recipe", "result": "allowed_invalid_data"})
            except Exception:
                test_cases.append({"test": "invalid_recipe", "result": "properly_rejected"})
            
            # Test 2: Invalid step configuration
            try:
                invalid_step_id = str(uuid4())
                await self.db.execute_query("""
                    INSERT INTO valve_step_config (step_id, valve_number, duration_ms)
                    VALUES ($1, -1, -1000)
                """, [invalid_step_id])
                test_cases.append({"test": "invalid_valve_config", "result": "allowed_invalid_data"})
            except Exception:
                test_cases.append({"test": "invalid_valve_config", "result": "properly_rejected"})
            
            handles_errors = all(tc["result"] == "properly_rejected" for tc in test_cases)
            
            return {
                "handles_errors_gracefully": handles_errors,
                "test_cases": test_cases
            }
        
        except Exception as e:
            return {"handles_errors_gracefully": False, "error": str(e)}
    
    async def _test_concurrent_database_access(self) -> Dict[str, Any]:
        """Test concurrent database access"""
        try:
            # Simulate concurrent operations
            concurrent_operations = []
            
            for i in range(5):
                operation = self.db.execute_query("SELECT COUNT(*) FROM recipes")
                concurrent_operations.append(operation)
            
            # Execute concurrently
            results = await asyncio.gather(*concurrent_operations, return_exceptions=True)
            
            # Check for deadlocks or errors
            errors = [r for r in results if isinstance(r, Exception)]
            successful = len(results) - len(errors)
            
            return {
                "no_deadlocks": len(errors) == 0,
                "data_consistency": successful == len(results),
                "successful_operations": successful,
                "errors": len(errors)
            }
        
        except Exception as e:
            return {"no_deadlocks": False, "data_consistency": False, "error": str(e)}
    
    async def _perform_system_health_check(self) -> Dict[str, Any]:
        """Perform comprehensive system health check"""
        try:
            health_checks = {}
            
            # Database connectivity
            try:
                await self.db.execute_query("SELECT 1")
                health_checks["database"] = "healthy"
            except Exception as e:
                health_checks["database"] = f"unhealthy: {str(e)}"
            
            # PLC manager status
            try:
                if hasattr(test_env, 'plc_manager') and test_env.plc_manager:
                    health_checks["plc_manager"] = "healthy" 
                else:
                    health_checks["plc_manager"] = "not_initialized"
            except Exception as e:
                health_checks["plc_manager"] = f"unhealthy: {str(e)}"
            
            # Data integrity
            try:
                result = await self.db.execute_query("SELECT COUNT(*) FROM recipes")
                if result[0]["count"] > 0:
                    health_checks["data_integrity"] = "healthy"
                else:
                    health_checks["data_integrity"] = "no_test_data"
            except Exception as e:
                health_checks["data_integrity"] = f"unhealthy: {str(e)}"
            
            # Overall health assessment
            unhealthy_components = [k for k, v in health_checks.items() if "unhealthy" in str(v)]
            
            if not unhealthy_components:
                overall_health = "healthy"
            elif len(unhealthy_components) < len(health_checks) / 2:
                overall_health = "degraded"
            else:
                overall_health = "unhealthy"
            
            return {
                "overall_health": overall_health,
                "component_health": health_checks,
                "unhealthy_components": unhealthy_components
            }
        
        except Exception as e:
            return {"overall_health": "unhealthy", "error": str(e)}

# Global test suite instance
test_suite = ComprehensiveTestSuite()

async def main():
    """Main test suite execution"""
    print("ðŸ§ª Starting Comprehensive Test Suite...")
    
    try:
        # Initialize test suite
        if not await test_suite.initialize():
            print("âŒ Test suite initialization failed")
            return
        
        # Run complete test suite
        metrics = await test_suite.run_complete_test_suite()
        
        # Print summary
        print("\n" + "="*60)
        print("ðŸŽ¯ TEST SUITE EXECUTION COMPLETE")
        print("="*60)
        print(f"Status: {metrics.status}")
        print(f"Total Tests: {metrics.total_tests}")
        print(f"Passed: {metrics.passed_tests}")
        print(f"Failed: {metrics.failed_tests}")
        print(f"Skipped: {metrics.skipped_tests}")
        print(f"Success Rate: {metrics.success_rate:.1%}")
        print(f"Duration: {metrics.total_duration_seconds:.2f} seconds")
        print("="*60)
        
        if metrics.status == "PASSED":
            print("âœ… System ready for production!")
        else:
            print("âŒ System requires attention before production deployment")
            
    except Exception as e:
        print(f"âŒ Test suite execution failed: {e}")
        traceback.print_exc()
    finally:
        # Cleanup
        if hasattr(test_env, 'cleanup_environment'):
            await test_env.cleanup_environment()

if __name__ == "__main__":
    asyncio.run(main())